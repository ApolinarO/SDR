x ->(1,2,3,4)
x->(1,2,3,4)
x <- c(1,2,3)
y<- c(4,5,6)
x+y
q()
plot(x1,x2)
set.seed(1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
set.seed(1)
x1=runif(100)
x2=0.5*x1+rnorm(100)/10
y=2+2*x1+0.3*x2+rnorm(100)
lr2.fit = lm(y~x1+x2)#y against x1 and x2. F-statistic of 12.8
summary(lr2.fit)
summary(lr3.fit)
lr4.fit = lm(y~x2) #y against x2. F-statstic of 20.98
lr4.fit = lm(y~x2) #y against x2. F-statstic of 20.98
summary(lr4.fit)
lr2.fit = lm(y~x1+x2)#y against x1 and x2. F-statistic of 12.8
summary(lr2.fit)
lr3.fit = lm(y~x1) #y against x1
summary(lr3.fit)
summary(lr2.fit)
lr4.fit = lm(y~x2) #y against x2. F-statstic of 20.98
summary(lr4.fit)
x1 <-c(x1,0.1)
x2 <-c(x2,0.8)
y<-c(y,6)
lr2.fit = lm(y~x1+x2)#y against x1 and x2.
summary(lr2.fit)
#new test y~x1+x2
##F-statistic of 13.72,R_sq:0.2188, x1_Coef_est:0.53, x2_Coef_est 2.51
lr3.fit = lm(y~x1) #y against x1
summary(lr3.fit)
##F-statistic of 18.33,R_sq:0.1562, x1_Coef_est:1.76
lr4.fit = lm(y~x2) #y against x1
summary(lr4.fit)
##F-statistic of 26.66,, R_sq:0.2122, x2_Coef_est:3.11
plot(y)
plot(x1)
plot(x2)
library(ISLR)
Carseats
?Carseats
attach(Carseats)
Carseats <- ISLR::Carseats
#Fit a MLR to predict sales using Price, Urban, US
#change qualitative binaural predictors to 1s or 0
Carseats$Urban <- ifelse(Carseats$Urban == "Yes", 1, ifelse(Carseats$Urban == "No", 0, NA))
Carseats$US <- ifelse(Carseats$US == "Yes", 1, ifelse(Carseats$US == "No", 0, NA))
attach(Carseats)
#A)fit a multiple linear regression line on Price, Urban,US
mlr1 <- lm(Sales ~ Price + Urban + US)
summary(mlr1)
predict(mlr1,Carseats, interval = "confidence")
predict(mlr1,Carseats, interval = "prediction")
coef(mlr1)
#B Describe variables
#Urban was not significant, but Price and US were
#C fitting a smaller Multiple linear model using the significant variables
mlr2 <- lm(Sales ~ Price + US)
summary(mlr2)
coef(mlr2)
#comparing both models
#MLR1
#Multiple R-squared:  0.2393,	Adjusted R-squared:  0.2335
#F-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16
#MLR2
#Multiple R-squared:  0.2393,	Adjusted R-squared:  0.2354
#F-statistic: 62.43 on 2 and 397 DF,  p-value: < 2.2e-16
predict(mlr1,Carseats, interval = "confidence")
confint(mlr2)
summary(mlr2)
plot(Sales~Price + US)
plot(Sales~Price)
plot(Sales~Price + US)
plot(Sales~Price + US)
library(ISLR)
#problem 11 a-g
View(Auto)
#a)
#create a binomial category for mpg(below_med = 0, above_med = 1)
median(Auto$mpg)
Auto$mpg01[Auto$mpg > median(Auto$mpg)] <- 1
Auto$mpg01[Auto$mpg <= median(Auto$mpg)] <- 0
View(Auto)
#b)explore the new variable
attach(Auto)
plot(mpg01,weight)
plot(mpg01,horsepower)
boxplot(mpg01~cylinders)
table(mpg01,cylinders)
#c)
plot(mpg01,weight)
#c)
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit.mpg01 <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred.mpg01 <- predict(lda.fit.mpg01)
CM.mpg01 <- table(lda.pred.mpg01$class, Auto.test$mpg01)
CM.mpg01
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit.mpg01 <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred.mpg01 <- predict(lda.fit.mpg01,Auto.test$mpg01)
CM.mpg01 <- table(lda.pred.mpg01$class, Auto.test$mpg01)
CM.mpg01
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rat
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit. <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred <- predict(lda.fit,Auto.test)
CM <- table(lda.pred.mpg01$class, Auto.test$mpg01)
CM
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
lda.fit. <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred <- predict(lda.fit,Auto.test)
CM <- table(lda.pred.mpg01$class, Auto.test$mpg01)
CM
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
lda.fit. <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred <- predict(lda.fit,Auto.test)
CM <- table(lda.pred.mpg01$class, Auto.test$mpg01)
CM
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit. <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred <- predict(lda.fit,Auto.test)
CM <- table(lda.pred.mpg01, Auto.test$mpg01)
CM
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit. <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.fit.mpg01
lda.pred <- predict(lda.fit,Auto.test)
CM <- table(lda.pred$mpg01, Auto.test$mpg01)
CM
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#perform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit, Auto.test)
CM <- table(lda.pred$mpg01, Auto.test$mpg01)
CM.mpg01
(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
length(lda.pred$mpg01)
length(Auto.test$mpg01)
length(lda.pred)
lda.pred <- predict(lda.fit, Auto.test)
length(lda.pred)
lda.pred
length(lda.pred$class)
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit$class, Auto.test$mpg)
CM <- table(lda.pred$mpg01, Auto.test$mpg01)
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit$class, Auto.test$mpg01)
CM <- table(lda.pred$mpg01, Auto.test$mpg01)
lds.test.error <-(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit$class, Auto.test$mpg01)
CM <- table(lda.pred$mpg01, Auto.test$mpg01)
lds.test.error <-(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit$class, Auto.test$mpg01)
CM <- table(lda.pred, Auto.test$mpg01)
lds.test.error <-(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit$class, Auto.test$mpg01)
lda.fit
lda.fit$class
lda.pred <- predict(lda.fit, Auto.test)
library(MASS)
set.seed(1011)
train<-sort(sample(1:dim(Auto)[1],196))
#split into training/test dataset
Auto.test <- Auto[-train, ]
Auto.train <- Auto[train,]
#peldrform qda
lda.fit <- lda(mpg01 ~ cylinders + weight, data = Auto.train)
lda.pred <- predict(lda.fit, Auto.test)
CM <- table(lda.pred$class, Auto.test$mpg01)
lds.test.error <-(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
lda.test.error <-(CM[1,2] + CM[2,1])/sum(CM) * 100 #7.65 % error rate
lda.test.error
#perform qda
qda.fit.mpg01 <- qda(mpg01 ~ cylinders + weight, data = Auto.train)
qda.fit.mpg01
qda.pred.mpg01 <- predict(qda.fit.mpg01)
CM.mpg01 <- table(qda.pred.mpg01$class, Auto.train$mpg01)
CM.mpg01
(CM.mpg01[1,2] + CM.mpg01[2,1])/sum(CM.mpg01) * 100 #8.67 % error rate
#perform qda
qda.fit<- qda(mpg01 ~ cylinders + weight, data = Auto.train)
qda.fit
qda.pred <- predict(qda.fit,Auto.test)
CM <- table(qda.pred$class, Auto.test$mpg01)
qda.test.error <- (CM[1,2] + CM[2,1])/sum(CM) * 100 #8.67 % error rate
qda.test.error
data = read.csv("train.csv")
library(tree)
library(randomForest)
setwd("/home/tony/R/work/Puerto_kaggle_insurance")
data = read.csv("train.csv")
dummy_data = data[1:500,] #dummy training data...need it to speed up basic operations
attach(dummy_data)
dummy.target <- ifelse(dummy_data$target == 1,"Yes", "No")
cbind(dummy_data$target, dummy.target)
dummy_data <- data.frame(dummy_data, dummy.target)
dummy_data$id <-NULL #id had some affect on the tree
dummy_data$target <- NULL
train <- sample(1:nrow(dummy_data), nrow(dummy_data)/2)
tree.dummy <- tree(dummy.target ~., subset = train, data = dummy_data)
summary(tree.dummy)
dummy.test <- dummy_data[-train, ]
tree.yhat <- predict(tree.dummy, newdata = dummy.test,type="class")
dummy.test.target <- dummy.test$dummy.target
cbind(yhat, dummy.test.target)[1:20,]
table(yhat, dummy.test.target)
cbind(tree.yhat, dummy.test.target)[1:20,]
table(tree.yhat, dummy.test.target)
bag.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 58)
bag.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 57)
importance(bag.dummy)
varImpPlot(bag.dummy)
yhat.bag <- predict(bag.dummy, newdata = dummy_data[-train,])
cbind(yhat.bag, dummy.test.target)
table(yhat.bag, dummy.test.target)
rf.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 20)
importance(rf.dummy)
varImpPlot(rf.dummy)
yhat.rf <- predict(rf.dummy, newdata = dummy_data[-train,])
cbind(yhat.rf, dummy.test.target)
table(yhat.rf, dummy.test.target)
varImpPlot(rf.dummy)
importance(rf.dummy)
rf.gini <- varImpPlot(rf.dummy)
max(rf.gini)
max(rf.gini)[1:5]
rf.gini <- order(varImpPlot(rf.dummy))
rf.gini
rf.gini <- (varImpPlot(rf.dummy))
rf.gini
rf.gini$MeanDecreasGini
rf.gini$MeanDecreaseGini
str(rf.gini)
rf.gini$MeanDecreaseGini
rf.gini[rf.gini$MeanDecreaseGini]
rf.gini[,MeanDecreaseGini]
rf.gini[,'MeanDecreaseGini']
order(rf.gini[,'MeanDecreaseGini'])
rf.gini[,order('MeanDecreaseGini')]
rf.gini[,order('MeanDecreaseGini')]
rf.gini[,'MeanDecreaseGini']
max(rf.gini[,'MeanDecreaseGini'])
order(rf.gini[,'MeanDecreaseGini'])
rf.gini[order(rf.gini[,'MeanDecreaseGini'])][1:5]
rf.gini[order(rf.gini[,'MeanDecreaseGini'])]
rf.gini[order(decreasing = TRUE)(rf.gini[,'MeanDecreaseGini'])]
rf.gini[order(rf.gini[,'MeanDecreaseGini'],decreasing = TRUE)]
rf.gini[order(rf.gini['','MeanDecreaseGini'],decreasing = TRUE)]
rf.gini[order(rf.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
str(rf.gini)
rf.gini[order(rf.gini[1:5,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
rf.gini[order(rf.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
table(yhat.rf, dummy.test.target)
table(yhat.bag, dummy.test.target)
table(tree.yhat, dummy.test.target)
prune.tree <- prune.misclass(tree.dummy, best = 7)
plot(prune.tree)
text(prune.tree, pretty = 0)
prune.tree.yhat <- predict(prune.tree, newdata = dummy.test,type="class")
cbind(prune.tree.yhat, dummy.test.target)[1:20,]
table(prune.tree.yhat, dummy.test.target)
prune.bag <- prune.misclass(bag.dummy, best = 7)
table(prune.tree.yhat, dummy.test.target)
table(prune.tree.yhat, tree.yhat)
prune.tree <- prune.misclass(tree.dummy, best = 3)
plot(prune.tree)
text(prune.tree, pretty = 0)
prune.tree <- prune.misclass(tree.dummy, best = 3)
text(prune.tree, pretty = 0)
prune.tree.yhat <- predict(prune.tree, newdata = dummy.test,type="class")
cbind(prune.tree.yhat, dummy.test.target)[1:20,]
table(prune.tree.yhat, dummy.test.target)
table(prune.tree.yhat, dummy.test.target)
table(prune.tree.yhat, tree.yhat) ##pruning did not affect anything??? why
prune.tree <- prune.misclass(tree.dummy, best = 3)
text(prune.tree, pretty = 0)
dummy_data = data[1:5000,] #dummy training data...need it to speed up basic operations
dummy.target <- ifelse(dummy_data$target == 1,"Yes", "No")
cbind(dummy_data$target, dummy.target)
dummy_data <- data.frame(dummy_data, dummy.target)
dummy_data$id <-NULL #id had some affect on the tree
dummy_data$target <- NULL
train <- sample(1:nrow(dummy_data), nrow(dummy_data)/10)
tree.dummy <- tree(dummy.target ~., subset = train, data = dummy_data)
summary(tree.dummy)
tree.dummy
dummy.test <- dummy_data[-train, ]
dummy.test
tree.yhat <- predict(tree.dummy, newdata = dummy.test,type="class")
dummy.test.target <- dummy.test$dummy.target
dummy.test.target
cbind(tree.yhat, dummy.test.target)[1:20,]
table(tree.yhat, dummy.test.target)
prune.tree <- prune.misclass(tree.dummy, best = 3)
plot(prune.tree)
text(prune.tree, pretty = 0)
prune.tree.yhat <- predict(prune.tree, newdata = dummy.test,type="class")
cbind(prune.tree.yhat, dummy.test.target)[1:20,]
table(prune.tree.yhat, dummy.test.target)
table(prune.tree.yhat, tree.yhat) ##pruning did not affect anything??? why
bag.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 57)
importance(bag.dummy)
varImpPlot(bag.dummy)
bag.gini <- (varImpPlot(bag.dummy))
bag.gini[order(bag.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
yhat.bag <- predict(bag.dummy, newdata = dummy_data[-train,])
cbind(yhat.bag, dummy.test.target)
table(yhat.bag, dummy.test.target)
rf.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 20)
importance(rf.dummy)
rf.gini <- (varImpPlot(rf.dummy))
str(rf.gini)
rf.gini[order(rf.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
yhat.rf <- predict(rf.dummy, newdata = dummy_data[-train,])
cbind(yhat.rf, dummy.test.target)
table(yhat.rf, dummy.test.target)
varImpPlot(tree.dummy)
dummy_data = data[1:50000,] #dummy training data...need it to speed up basic operations
dummy.target <- ifelse(dummy_data$target == 1,"Yes", "No")
cbind(dummy_data$target, dummy.target)
dummy_data <- data.frame(dummy_data, dummy.target)
dummy_data$id <-NULL #id had some affect on the tree
dummy_data$target <- NULL
train <- sample(1:nrow(dummy_data), nrow(dummy_data)/10)
tree.dummy <- tree(dummy.target ~., subset = train, data = dummy_data)
dummy.test <- dummy_data[-train, ]
dummy.test
tree.yhat <- predict(tree.dummy, newdata = dummy.test,type="class")
dummy.test.target <- dummy.test$dummy.target
dummy.test.target
cbind(tree.yhat, dummy.test.target)[1:20,]
table(tree.yhat, dummy.test.target)
prune.tree <- prune.misclass(tree.dummy, best = 7)
plot(prune.tree)
text(prune.tree, pretty = 0)
prune.tree.yhat <- predict(prune.tree, newdata = dummy.test,type="class")
cbind(prune.tree.yhat, dummy.test.target)[1:20,]
table(prune.tree.yhat, dummy.test.target)
prune.tree <- prune.misclass(tree.dummy, best = 7)
plot(prune.tree)
text(prune.tree, pretty = 0)
plot(tree.dummy)
text(tree,dummy, pretty = 0)
text(tree.dummy, pretty = 0)
tree.dummy <- tree(dummy.target ~., subset = train, data = dummy_data)
bag.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 57)
importance(bag.dummy)
bag.gini <- (varImpPlot(bag.dummy))
bag.gini[order(bag.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
rf.dummy <- randomForest(dummy.target ~., subset = train, data = dummy_data, mtry = 20)
train, data = dummy_data, mtry = 20)
importance(rf.dummy)
rf.gini <- (varImpPlot(rf.dummy))
str(rf.gini)
rf.gini[order(rf.gini[,'MeanDecreaseGini'],decreasing = TRUE)][1:5]
yhat.rf <- predict(rf.dummy, newdata = dummy_data[-train,])
cbind(yhat.rf, dummy.test.target)
table(yhat.rf, dummy.test.target)
yhat.bag <- predict(bag.dummy, newdata = dummy_data[-train,])
cbind(yhat.bag, dummy.test.target)
table(yhat.bag, dummy.test.target)
prune.tree.yhat <- predict(prune.tree, newdata = dummy.test,type="class")
cbind(prune.tree.yhat, dummy.test.target)[1:20,]
table(prune.tree.yhat, dummy.test.target)
install.packages("shiny")
library(shiny)
runExample("01_hello")
cw
cd
getwd
getwd
dir <- getwd()
dir
setwd("/home/tony/R/work/shiny")
setwd("/home/tony/R/work/shiny/App-1")
setwd("/home/tony/R/work/shiny/")
runApp("App-1")
source('~/R/work/Puerto_kaggle_insurance/shiny_demo.R')
runExample("04_mpg")        # global variables
library(shiny)
library(Mass)
library(MASS)
data <- Boston
view(data)
View(data)
str(data)
hist(zn)
hist(chas)
attach(data)
hist(chas)
?data
?Boston
source('~/R/work/shiny/Boston/app.r', echo=TRUE)
data <- Auto
View(data)
summary(data)
min(data$year)
max(data$year)
runApp("Auto")
runApp('Auto')
runExample("05_sliders")
unExample("03_reactivity")
runExample("03_reactivity")
runApp("Auto")
